Parameters:
  BucketName:
    Description: Bucket Name
    Type: String
    Default: "fao-awsfoodagg"
  LambdaFunctionName:
    Description: LambdaFunctionName
    Type: String
Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: 'Delete'
    Properties:
      BucketName: !Ref BucketName
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
  LambdaEMR:
    Type: AWS::Lambda::Function
    Properties:
      Description: Create an EMR cluster
      FunctionName: !Ref LambdaFunctionName
      Handler: index.lambda_handler
      MemorySize: 128
      Runtime: python3.9
      Role: !GetAtt 'LambdaRole.Arn'
      Timeout: 240
      Code:
        ZipFile: |
            """
            Copy paste the following code in your Lambda function. Make sure to change the following key parameters for the API as per your account

            -Name (Name of Spark cluster)
            -LogUri (S3 bucket to store EMR logs)
            -Ec2SubnetId (The subnet to launch the cluster into)
            -JobFlowRole (Service role for EC2)
            -ServiceRole (Service role for Amazon EMR)
            
            The following parameters are additional parameters for the Spark job itself. Change the bucket name and prefix for the Spark job (located at the bottom).
            
            -s3://your-bucket-name/prefix/lambda-emr/SparkProfitCalc.jar (Spark jar file)
            -s3://your-bucket-name/prefix/fake_sales_data.csv (Input data file in S3)
            -s3://your-bucket-name/prefix/outputs/report_1/ (Output location in S3)
            """
            import json
            import boto3


            client = boto3.client('emr')
            
            
            def lambda_handler(event, context):
               
                response = client.run_job_flow(
                    Name= 'FaoFoodAgg',
                    LogUri= 's3://fao-awsfoodaggparam/prefix/logs',
                    ReleaseLabel= 'emr-6.9.0',
                    Instances={
                        'MasterInstanceType': 'm5.xlarge',
                        'SlaveInstanceType': 'm5.large',
                        'InstanceCount': 1,
                        'KeepJobFlowAliveWhenNoSteps': False,
                        'TerminationProtected': False,
                        'Ec2SubnetId': 'subnet-72a5a414'
                    },
                    Applications = [ {'Name': 'Spark'} ],
                    Configurations = [
                        { 'Classification': 'spark-hive-site',
                          'Properties': {
                              'hive.metastore.client.factory.class': 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'}
                        }
                    ],
                    VisibleToAllUsers=True,
                    JobFlowRole = 'EMR_EC2_DefaultRole',
                    ServiceRole = 'EMR_DefaultRole',
                    Steps=[
                        {
                            'Name': 'fao-data-analysis',
                            'ActionOnFailure': 'TERMINATE_CLUSTER',
                            'HadoopJarStep': {
                                    'Jar': 'command-runner.jar',
                                    'Args': [
                                        'spark-submit',
                                        '--deploy-mode', 'cluster',
                                        '--executor-memory', '6G',
                                        '--num-executors', '1',
                                        '--executor-cores', '2',
                                        '--class', 'com.jivega.awsfoodagg.driver.Driver',
                                        's3://codepipelineartifact-aws-common/deploy/awsfoodagg_2.13-0.1.0-SNAPSHOT.jar',
                                        's3://fao-awsfoodaggparam/prefix/fake_sales_data.csv',
                                        's3://fao-awsfoodaggparam/prefix/outputs/report_1/'
                                    ]
                            }
                        }
                    ]
                )
            
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: "LambdaEMRRoleName"
      Description: An execution role for a Lambda function launched by CloudFormation
      ManagedPolicyArns:
        - !Ref LambdaPolicy
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action:
          - 'sts:AssumeRole'                

  LambdaPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: "AWSfoodAggLambda"
      Description: Managed policy for a Lambda function launched by CloudFormation
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - 'logs:CreateLogStream'
              - 'logs:PutLogEvents'
            Resource: !Join ['',['arn:', !Ref AWS::Partition, ':logs:', !Ref AWS::Region, ':', !Ref AWS::AccountId, ':log-group:/aws/lambda/', !Ref LambdaFunctionName, ':*']]
          - Effect: Allow
            Action:
              - 'logs:CreateLogGroup'
            Resource: !Sub 'arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*'
          - Effect: Allow
            Action:
              - 'elasticmapreduce:RunJobFlow'
            Resource: '*'
          - Effect: Allow
            Action:
              - 'iam:PassRole'
            Resource: '*' 
            
            